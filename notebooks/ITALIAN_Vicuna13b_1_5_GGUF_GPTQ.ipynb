{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabiomatricardi/AI-RewriteText/blob/main/notebooks/ITALIAN_Vicuna13b_1_5_GGUF_GPTQ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ITALIAN Vicuna13b TESTS\n",
        "\n",
        "---\n",
        "\n",
        "After seeing Vicuna33b replying to Italian questions on https://huggingface.co/spaces/lmsys/chatbot-arena"
      ],
      "metadata": {
        "id": "oChUSl2LnqYC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "Chatbot Arena\n",
        "Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions.\n",
        "\n",
        "\n",
        "### Prompt Battle judge\n",
        "- uses an LLM to judge the prompts https://arxiv.org/pdf/2306.05685.pdf\n",
        "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena\n",
        "https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard\n"
      ],
      "metadata": {
        "id": "5CK3IT7pX8EC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/fabiomatricardi/AI-RewriteText/raw/main/texts/tomp3.cc%20-%20Mistral%207B%20%20The%20New%207B%20LLaMA%20Killer_1080p.txt\n",
        "!wget https://github.com/fabiomatricardi/AI-RewriteText/raw/main/texts/tomp3.cc%20-%20This%20AI%20Startup%20ACTUALLY%20Opens%20AI%20%20starts%20with%20POWERFUL%207B%20LLM_1080p.txt"
      ],
      "metadata": {
        "id": "O5PpuCRWAxgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GGML section\n",
        "using TheBloke/vicuna-13B-v1.5-GGML\n"
      ],
      "metadata": {
        "id": "Uwg6SmyfZqvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install ctransformers>=0.2.24\n",
        "!pip install langchain\n",
        "!pip install rich\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "QbwBrX3YZqKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/TheBloke/vicuna-13B-v1.5-GGML/resolve/main/vicuna-13b-v1.5.ggmlv3.q4_K_M.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiO41tw1ZVTS",
        "outputId": "a9c6efcf-c77d-4edf-ce17-721f4d736fa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-13 23:38:23--  https://huggingface.co/TheBloke/vicuna-13B-v1.5-GGML/resolve/main/vicuna-13b-v1.5.ggmlv3.q4_K_M.bin\n",
            "Resolving huggingface.co (huggingface.co)... 18.172.134.88, 18.172.134.124, 18.172.134.4, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.172.134.88|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/7e/81/7e818b49dedd66a09da83a8729191a51055937ab6219c73cbc2c966a7427d215/5fe5ec7c23d08a961cd0e1bbc4944ba36254c887a50a5d94ce173c488da10a9e?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27vicuna-13b-v1.5.ggmlv3.q4_K_M.bin%3B+filename%3D%22vicuna-13b-v1.5.ggmlv3.q4_K_M.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1697499503&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5NzQ5OTUwM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy83ZS84MS83ZTgxOGI0OWRlZGQ2NmEwOWRhODNhODcyOTE5MWE1MTA1NTkzN2FiNjIxOWM3M2NiYzJjOTY2YTc0MjdkMjE1LzVmZTVlYzdjMjNkMDhhOTYxY2QwZTFiYmM0OTQ0YmEzNjI1NGM4ODdhNTBhNWQ5NGNlMTczYzQ4OGRhMTBhOWU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=OfJl0byrqZ3eL3gPYrHu3O8ct7u0p9X72SQWoEpwl6yzE5yZzAl3-FIk-mQ0x5jifHhu51d1AmpqtUZ4wNSaI0PS%7E5QqP4PdJrpWMsjXq%7Ebxa%7EaFSjp1YMhl6NXMtSjMuUcQvDVwdU7vp9MFXTPctDMUvZFV1HA9f0ox1NDkwK3SRzuF7FL2IP889NKl7yn6Kh56WrSgUG1CJi2-nGybT92F5yWPXLDlfVnliv30SvIKn84iJS0Sku0I%7EsnPTCGwCil7NpnYZMTPqGFJ42tX1gXnTWfD7U2uoXpKwaf7ru9gn3nxIpE874Cxw1RKpZnCWyXaNAt1oLhzcZRtbKWVdA__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-10-13 23:38:23--  https://cdn-lfs.huggingface.co/repos/7e/81/7e818b49dedd66a09da83a8729191a51055937ab6219c73cbc2c966a7427d215/5fe5ec7c23d08a961cd0e1bbc4944ba36254c887a50a5d94ce173c488da10a9e?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27vicuna-13b-v1.5.ggmlv3.q4_K_M.bin%3B+filename%3D%22vicuna-13b-v1.5.ggmlv3.q4_K_M.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1697499503&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5NzQ5OTUwM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy83ZS84MS83ZTgxOGI0OWRlZGQ2NmEwOWRhODNhODcyOTE5MWE1MTA1NTkzN2FiNjIxOWM3M2NiYzJjOTY2YTc0MjdkMjE1LzVmZTVlYzdjMjNkMDhhOTYxY2QwZTFiYmM0OTQ0YmEzNjI1NGM4ODdhNTBhNWQ5NGNlMTczYzQ4OGRhMTBhOWU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=OfJl0byrqZ3eL3gPYrHu3O8ct7u0p9X72SQWoEpwl6yzE5yZzAl3-FIk-mQ0x5jifHhu51d1AmpqtUZ4wNSaI0PS%7E5QqP4PdJrpWMsjXq%7Ebxa%7EaFSjp1YMhl6NXMtSjMuUcQvDVwdU7vp9MFXTPctDMUvZFV1HA9f0ox1NDkwK3SRzuF7FL2IP889NKl7yn6Kh56WrSgUG1CJi2-nGybT92F5yWPXLDlfVnliv30SvIKn84iJS0Sku0I%7EsnPTCGwCil7NpnYZMTPqGFJ42tX1gXnTWfD7U2uoXpKwaf7ru9gn3nxIpE874Cxw1RKpZnCWyXaNAt1oLhzcZRtbKWVdA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.154.185.64, 18.154.185.94, 18.154.185.27, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.154.185.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7865666688 (7.3G) [application/octet-stream]\n",
            "Saving to: ‘vicuna-13b-v1.5.ggmlv3.q4_K_M.bin’\n",
            "\n",
            "vicuna-13b-v1.5.ggm 100%[===================>]   7.33G  43.1MB/s    in 3m 17s  \n",
            "\n",
            "2023-10-13 23:41:40 (38.1 MB/s) - ‘vicuna-13b-v1.5.ggmlv3.q4_K_M.bin’ saved [7865666688/7865666688]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Restart Runtime {display-mode: \"form\"}\n",
        "import ipywidgets as widgets\n",
        "def restart(b):\n",
        "  exit()\n",
        "\n",
        "button2 = widgets.Button(\n",
        "    description='Restart Runtime',\n",
        "    disabled=False,\n",
        "    button_style='warning', # 'success', 'info', 'warning', 'danger' or ''\n",
        "    tooltip='Click me',\n",
        "    icon='check' # (FontAwesome names without the `fa-` prefix)\n",
        ")\n",
        "button2.on_click(restart)\n",
        "button2\n"
      ],
      "metadata": {
        "id": "-hsc5IfVET8S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "3bf67f2389904dc7ae213d20f61cca49",
            "b9ef112dc4e648f786c53881e546b07a",
            "08a3dbac302048278e3b095c0e996901"
          ]
        },
        "outputId": "e3389bcb-8cef-4798-8e7e-594c8894a505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='warning', description='Restart Runtime', icon='check', style=ButtonStyle(), tooltip='Clic…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3bf67f2389904dc7ae213d20f61cca49"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rich import print\n",
        "from tqdm.rich import trange, tqdm\n",
        "from rich import console\n",
        "from rich.panel import Panel\n",
        "from rich.markdown import Markdown\n",
        "from rich.text import Text\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "from ctransformers import AutoModelForCausalLM, AutoConfig, Config\n",
        "import datetime\n",
        "from rich.console import Console\n",
        "console = Console(width=110)"
      ],
      "metadata": {
        "id": "ryXI1qdEzk64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conf = AutoConfig(Config(temperature=0.7, repetition_penalty=1.1, batch_size=52,\n",
        "                max_new_tokens=1024, context_length=1024))\n",
        "llm = AutoModelForCausalLM.from_pretrained(\"/content/vicuna-13b-v1.5.ggmlv3.q4_K_M.bin\",\n",
        "                                           model_type=\"llama\", config = conf)"
      ],
      "metadata": {
        "id": "eRJCb4FxzwVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FUNCTION TO LOG ALL CHAT MESSAGES INTO chathistory.txt\n",
        "def writehistory(text):\n",
        "    with open('20231014_italian_Vicuna13b-history.txt', 'a') as f:\n",
        "        f.write(text)\n",
        "        f.write('\\n')\n",
        "    f.close()\n",
        "\n",
        "def vicunaQ4KM_CT(prompt):\n",
        "  from rich.markdown import Markdown\n",
        "  #t_vicuna = f\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: {prompt} ASSISTANT:\"\n",
        "  t_vicuna = f\"USER: {prompt} ASSISTANT:\"\n",
        "  start = datetime.datetime.now()\n",
        "  console.print(f\"[italic bold bright_red]Prompt: {prompt}\")\n",
        "  answer = llm(t_vicuna, temperature = 0.7, repetition_penalty = 1.15,\n",
        "             max_new_tokens = 2048)\n",
        "  stop = datetime.datetime.now()\n",
        "  tok2 = len(llm.tokenize(t_vicuna))\n",
        "  tok1 = len(llm.tokenize(answer))\n",
        "  #console.print(f\"[italic]Number of characters in orginal prompt: {len(prompt)}\") #it works with CTransformers without Langchain\n",
        "  console.print(f\"[italic bold]Number of tokens in the  prompt: {tok2}\") #it works with CTransformers without Langchain\n",
        "  console.print(f\"[italic bold]Number of tokens in the  answer: {tok1}\")\n",
        "  console.print(Markdown(answer))\n",
        "  console.print(f\"[bold italic green] Generated by Vicuna-13b in {stop-start}\")\n",
        "  text = f\"user: {prompt}\\nVicuna-7b: {answer}\\nGenerated in {stop-start}\"\n",
        "  writehistory(text)\n",
        "  console.print(f\"[blue1] ---\")\n",
        "  return answer"
      ],
      "metadata": {
        "id": "rTKihE9gaD7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test with Itlian prompts"
      ],
      "metadata": {
        "id": "YISUEwtTal6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rich.prompt import Prompt"
      ],
      "metadata": {
        "id": "odpNg1LqcBKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "database = []"
      ],
      "metadata": {
        "id": "Etzk3GBacddx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = input(\"La tua domanda: \")\n",
        "prompt1 = vicunaQ4KM_CT(question)\n",
        "res = {'question': question,\n",
        "       'answer': prompt1}\n",
        "database.append(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "4zgwoEYGalTo",
        "outputId": "cce9acec-685e-4d70-eddc-eb9d9733957f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La tua domanda: cosa significa scienza?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;3;91mPrompt: cosa significa scienza?\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">Prompt: cosa significa scienza?</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;3mNumber of tokens in the  prompt: \u001b[0m\u001b[1;3;36m13\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold; font-style: italic\">Number of tokens in the  prompt: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold; font-style: italic\">13</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;3mNumber of tokens in the  answer: \u001b[0m\u001b[1;3;36m170\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold; font-style: italic\">Number of tokens in the  answer: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold; font-style: italic\">170</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "La scienza è un insieme di metodologie e conoscenze che permettono di comprendere il funzionamento del mondo  \n",
              "naturale, attraverso l'osservazione, la sperimentazione e l'analisi dei dati. La scienza si basa              \n",
              "sull'empirismo, ovvero sulla raccolta di dati osservativi, che vengono poi analizzati e interpretati per      \n",
              "formulate teorie e leggi scientifiche. La scienza è utilizzata in molte aree della vita quotidiana, dalla     \n",
              "medicina alla tecnologia, dall'ingegneria all'ambiente, e rappresenta uno degli strumenti più importanti per  \n",
              "comprendere il mondo che ci circonda e per risolvere i problemi attuali.                                      \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">La scienza è un insieme di metodologie e conoscenze che permettono di comprendere il funzionamento del mondo  \n",
              "naturale, attraverso l'osservazione, la sperimentazione e l'analisi dei dati. La scienza si basa              \n",
              "sull'empirismo, ovvero sulla raccolta di dati osservativi, che vengono poi analizzati e interpretati per      \n",
              "formulate teorie e leggi scientifiche. La scienza è utilizzata in molte aree della vita quotidiana, dalla     \n",
              "medicina alla tecnologia, dall'ingegneria all'ambiente, e rappresenta uno degli strumenti più importanti per  \n",
              "comprendere il mondo che ci circonda e per risolvere i problemi attuali.                                      \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;3;32m Generated by Vicuna-7b in \u001b[0m\u001b[1;3;32m0:03:00\u001b[0m\u001b[1;3;32m.\u001b[0m\u001b[1;3;32m437367\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold; font-style: italic\"> Generated by Vicuna-7b in 0:03:00.437367</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;5;21m ---\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff\"> ---</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"Quali sono le relazione tra AI e la filosofia per il futuro?\n",
        "Rispondi seguendo questo schema:\n",
        "RIFLESSIONI: // I tuoi pensieri qui\n",
        "CRITICHE: // Le critiche alle tue considerazioni\n",
        "RISPOSTA: // La tua risposta finale\"\"\"\n",
        "prompt1 = vicunaQ4KM_CT(question)\n",
        "res = {'question': question,\n",
        "       'answer': prompt1}\n",
        "database.append(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "gHcwkUs-gU0I",
        "outputId": "d9e2a597-4912-4b57-c506-9265592f130b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;3;91mPrompt: Quali sono le relazione tra AI e la filosofia per il futuro?\u001b[0m\n",
              "\u001b[1;3;91mRispondi seguendo questo schema:\u001b[0m\n",
              "\u001b[1;3;91mRIFLESSIONI: \u001b[0m\u001b[1;3;91m/\u001b[0m\u001b[1;3;91m/\u001b[0m\u001b[1;3;91m I tuoi pensieri qui\u001b[0m\n",
              "\u001b[1;3;91mCRITICHE: \u001b[0m\u001b[1;3;91m/\u001b[0m\u001b[1;3;91m/\u001b[0m\u001b[1;3;91m Le critiche alle tue considerazioni\u001b[0m\n",
              "\u001b[1;3;91mRISPOSTA: \u001b[0m\u001b[1;3;91m/\u001b[0m\u001b[1;3;91m/\u001b[0m\u001b[1;3;91m La tua risposta finale\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">Prompt: Quali sono le relazione tra AI e la filosofia per il futuro?</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">Rispondi seguendo questo schema:</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">RIFLESSIONI: // I tuoi pensieri qui</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">CRITICHE: // Le critiche alle tue considerazioni</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">RISPOSTA: // La tua risposta finale</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;3mNumber of tokens in the  prompt: \u001b[0m\u001b[1;3;36m78\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold; font-style: italic\">Number of tokens in the  prompt: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold; font-style: italic\">78</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;3mNumber of tokens in the  answer: \u001b[0m\u001b[1;3;36m369\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold; font-style: italic\">Number of tokens in the  answer: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold; font-style: italic\">369</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Riflessioni: L'AI ha una grande capacità di elaborazione delle informazioni e di apprendimento automatico, che\n",
              "la rende in grado di eseguire compiti complessi e di soluzione di problemi. Tuttavia, l'AI è soggetta a       \n",
              "limitazioni e errori, come la possibilità di avere pregiudizi o di commettere errori di valutazione. Inoltre, \n",
              "l'uso dell'AI può avere implicazioni etiche e sociali che devono essere prese in considerazione. Critiche: La \n",
              "relazione tra AI e filosofia per il futuro è complessa e multifacetata. Mentre alcuni filoni della filosofia, \n",
              "come l'etica e la teoria dell'intelligenza artificiale, sono direttamente coinvolti nell'analisi delle        \n",
              "implicazioni etiche e sociali dell'AI, altri sembrano meno interessati a questa tematica. Inoltre, alcune     \n",
              "critiche sostengono che la filosofia non ha alcun ruolo da svolgere nell'elaborazione dell'AI, in quanto le   \n",
              "sue capacità sono limitate dalla sua natura tecnologica e scientifica. Risposta: La relazione tra AI e        \n",
              "filosofia per il futuro è cruciale per affrontare le sfide etiche e sociali che l'uso dell'AI sta creando. La \n",
              "filosofia può contribuire all'analisi delle implicazioni della tecnologia sull'umanità, fornendo un quadro    \n",
              "teorico per comprendere i problemi etici e sociali associati all'AI. Inoltre, la filosofia può aiutare a      \n",
              "guidare il desenvol                                                                                           \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Riflessioni: L'AI ha una grande capacità di elaborazione delle informazioni e di apprendimento automatico, che\n",
              "la rende in grado di eseguire compiti complessi e di soluzione di problemi. Tuttavia, l'AI è soggetta a       \n",
              "limitazioni e errori, come la possibilità di avere pregiudizi o di commettere errori di valutazione. Inoltre, \n",
              "l'uso dell'AI può avere implicazioni etiche e sociali che devono essere prese in considerazione. Critiche: La \n",
              "relazione tra AI e filosofia per il futuro è complessa e multifacetata. Mentre alcuni filoni della filosofia, \n",
              "come l'etica e la teoria dell'intelligenza artificiale, sono direttamente coinvolti nell'analisi delle        \n",
              "implicazioni etiche e sociali dell'AI, altri sembrano meno interessati a questa tematica. Inoltre, alcune     \n",
              "critiche sostengono che la filosofia non ha alcun ruolo da svolgere nell'elaborazione dell'AI, in quanto le   \n",
              "sue capacità sono limitate dalla sua natura tecnologica e scientifica. Risposta: La relazione tra AI e        \n",
              "filosofia per il futuro è cruciale per affrontare le sfide etiche e sociali che l'uso dell'AI sta creando. La \n",
              "filosofia può contribuire all'analisi delle implicazioni della tecnologia sull'umanità, fornendo un quadro    \n",
              "teorico per comprendere i problemi etici e sociali associati all'AI. Inoltre, la filosofia può aiutare a      \n",
              "guidare il desenvol                                                                                           \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;3;32m Generated by Vicuna-7b in \u001b[0m\u001b[1;3;32m0:06:56\u001b[0m\u001b[1;3;32m.\u001b[0m\u001b[1;3;32m437224\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold; font-style: italic\"> Generated by Vicuna-7b in 0:06:56.437224</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;5;21m ---\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff\"> ---</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"Quali sono le relazione tra AI e apprendimento per il futuro?\n",
        "Rispondi seguendo questo schema:\n",
        "RIFLESSIONI: // I tuoi pensieri qui\n",
        "\n",
        "CRITICHE: // Le critiche alle tue considerazioni\n",
        "\n",
        "RISPOSTA: // La tua risposta finale\n",
        "\"\"\"\n",
        "prompt1 = vicunaQ4KM_CT(question)\n",
        "res = {'question': question,\n",
        "       'answer': prompt1}\n",
        "database.append(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "miMlSrenjVPd",
        "outputId": "acae9988-b7af-442a-b302-171bff828359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;3;91mPrompt: Quali sono le relazione tra AI e apprendimento per il futuro?\u001b[0m\n",
              "\u001b[1;3;91mRispondi seguendo questo schema:\u001b[0m\n",
              "\u001b[1;3;91mRIFLESSIONI: \u001b[0m\u001b[1;3;91m/\u001b[0m\u001b[1;3;91m/\u001b[0m\u001b[1;3;91m I tuoi pensieri qui\u001b[0m\n",
              "\n",
              "\u001b[1;3;91mCRITICHE: \u001b[0m\u001b[1;3;91m/\u001b[0m\u001b[1;3;91m/\u001b[0m\u001b[1;3;91m Le critiche alle tue considerazioni\u001b[0m\n",
              "\n",
              "\u001b[1;3;91mRISPOSTA: \u001b[0m\u001b[1;3;91m/\u001b[0m\u001b[1;3;91m/\u001b[0m\u001b[1;3;91m La tua risposta finale\u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">Prompt: Quali sono le relazione tra AI e apprendimento per il futuro?</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">Rispondi seguendo questo schema:</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">RIFLESSIONI: // I tuoi pensieri qui</span>\n",
              "\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">CRITICHE: // Le critiche alle tue considerazioni</span>\n",
              "\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">RISPOSTA: // La tua risposta finale</span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;3mNumber of tokens in the  prompt: \u001b[0m\u001b[1;3;36m81\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold; font-style: italic\">Number of tokens in the  prompt: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold; font-style: italic\">81</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;3mNumber of tokens in the  answer: \u001b[0m\u001b[1;3;36m429\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold; font-style: italic\">Number of tokens in the  answer: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold; font-style: italic\">429</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "RIFLESSIONI: L'AI e l'apprendimento automatico sono tecnologie in continua evoluzione che hanno il potenziale \n",
              "di trasformare molte aree della vita umana, dalle decisioni finanziarie alle cure mediche. Tuttavia, come     \n",
              "tutte le tecnologie, anche l'AI ha i suoi limiti e rischi. È importante considerare gli effetti a lungo       \n",
              "termine dell'uso dell'AI, comprese le questioni etiche che si presentano in questa area.                      \n",
              "\n",
              "CRITICHE: Una critica all'idea che l'AI possa sostituire completamente i dati di mercato è che l'AI non può   \n",
              "ancora prevedere il futuro con precisione e che ci sono molte variabili imprevisibili che possono influenzare \n",
              "le decisioni finanziarie. Inoltre, anche se l'AI può fornire valutazioni e raccomandazioni, la decisione      \n",
              "finale spetta sempre all'uomo.                                                                                \n",
              "\n",
              "RISPOSTA: Sono d'accordo con entrambe le posizioni presentate, ma credo che l'AI possa avere un impatto       \n",
              "significativo sulla formazione del futuro. L'AI può essere utilizzata per analizzare grandi quantità di dati e\n",
              "prevedere tendenze e cambiamenti nel mercato finanziario a breve e lungo termine, ma non può sostituire       \n",
              "completamente la comprensione umana delle decisioni finanziarie. Inoltre, l'AI può essere utilizzata per      \n",
              "creare modelli di simulazione che consentono agli investitori di esercitarsi e migliorare le loro capacità di \n",
              "previsione, senza dover affrontare i rischi reali del mercato finanziario. Tuttavia, è importante considerare \n",
              "anche gli effetti a lungo termine dell'uso dell'AI in questo campo e affrontare le questioni etiche che si    \n",
              "presentano in questa area.                                                                                    \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">RIFLESSIONI: L'AI e l'apprendimento automatico sono tecnologie in continua evoluzione che hanno il potenziale \n",
              "di trasformare molte aree della vita umana, dalle decisioni finanziarie alle cure mediche. Tuttavia, come     \n",
              "tutte le tecnologie, anche l'AI ha i suoi limiti e rischi. È importante considerare gli effetti a lungo       \n",
              "termine dell'uso dell'AI, comprese le questioni etiche che si presentano in questa area.                      \n",
              "\n",
              "CRITICHE: Una critica all'idea che l'AI possa sostituire completamente i dati di mercato è che l'AI non può   \n",
              "ancora prevedere il futuro con precisione e che ci sono molte variabili imprevisibili che possono influenzare \n",
              "le decisioni finanziarie. Inoltre, anche se l'AI può fornire valutazioni e raccomandazioni, la decisione      \n",
              "finale spetta sempre all'uomo.                                                                                \n",
              "\n",
              "RISPOSTA: Sono d'accordo con entrambe le posizioni presentate, ma credo che l'AI possa avere un impatto       \n",
              "significativo sulla formazione del futuro. L'AI può essere utilizzata per analizzare grandi quantità di dati e\n",
              "prevedere tendenze e cambiamenti nel mercato finanziario a breve e lungo termine, ma non può sostituire       \n",
              "completamente la comprensione umana delle decisioni finanziarie. Inoltre, l'AI può essere utilizzata per      \n",
              "creare modelli di simulazione che consentono agli investitori di esercitarsi e migliorare le loro capacità di \n",
              "previsione, senza dover affrontare i rischi reali del mercato finanziario. Tuttavia, è importante considerare \n",
              "anche gli effetti a lungo termine dell'uso dell'AI in questo campo e affrontare le questioni etiche che si    \n",
              "presentano in questa area.                                                                                    \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;3;32m Generated by Vicuna-7b in \u001b[0m\u001b[1;3;32m0:07:59\u001b[0m\u001b[1;3;32m.\u001b[0m\u001b[1;3;32m796733\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold; font-style: italic\"> Generated by Vicuna-7b in 0:07:59.796733</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;5;21m ---\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff\"> ---</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"Traduci in lingua Inglese il seguente testo delimitato da tre backquotes:\n",
        "```\n",
        "RIFLESSIONI: L'AI e l'apprendimento automatico sono tecnologie in continua evoluzione che hanno il potenziale\n",
        "di trasformare molte aree della vita umana, dalle decisioni finanziarie alle cure mediche. Tuttavia, come\n",
        "tutte le tecnologie, anche l'AI ha i suoi limiti e rischi. È importante considerare gli effetti a lungo\n",
        "termine dell'uso dell'AI, comprese le questioni etiche che si presentano in questa area.\n",
        "\n",
        "CRITICHE: Una critica all'idea che l'AI possa sostituire completamente i dati di mercato è che l'AI non può\n",
        "ancora prevedere il futuro con precisione e che ci sono molte variabili imprevisibili che possono influenzare\n",
        "le decisioni finanziarie. Inoltre, anche se l'AI può fornire valutazioni e raccomandazioni, la decisione\n",
        "finale spetta sempre all'uomo.\n",
        "\n",
        "RISPOSTA: Sono d'accordo con entrambe le posizioni presentate, ma credo che l'AI possa avere un impatto\n",
        "significativo sulla formazione del futuro. L'AI può essere utilizzata per analizzare grandi quantità di dati e\n",
        "prevedere tendenze e cambiamenti nel mercato finanziario a breve e lungo termine, ma non può sostituire\n",
        "completamente la comprensione umana delle decisioni finanziarie. Inoltre, l'AI può essere utilizzata per\n",
        "creare modelli di simulazione che consentono agli investitori di esercitarsi e migliorare le loro capacità di\n",
        "previsione, senza dover affrontare i rischi reali del mercato finanziario. Tuttavia, è importante considerare\n",
        "anche gli effetti a lungo termine dell'uso dell'AI in questo campo e affrontare le questioni etiche che si\n",
        "presentano in questa area.\n",
        "```\n",
        "\"\"\"\n",
        "prompt1 = vicunaQ4KM_CT(question)\n",
        "res = {'question': question,\n",
        "       'answer': prompt1}\n",
        "database.append(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "wSWAk8fNllZr",
        "outputId": "61c2681b-7c52-47ca-fbd2-2064b5e48686"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;3;91mPrompt: Traduci in lingua Inglese il seguente testo delimitato da tre backquotes:\u001b[0m\n",
              "\u001b[1;3;91m```\u001b[0m\n",
              "\u001b[1;3;91mRIFLESSIONI: L'AI e l'apprendimento automatico sono tecnologie in continua evoluzione che hanno il potenziale \u001b[0m\n",
              "\u001b[1;3;91mdi trasformare molte aree della vita umana, dalle decisioni finanziarie alle cure mediche. Tuttavia, come     \u001b[0m\n",
              "\u001b[1;3;91mtutte le tecnologie, anche l'AI ha i suoi limiti e rischi. È importante considerare gli effetti a lungo       \u001b[0m\n",
              "\u001b[1;3;91mtermine dell'uso dell'AI, comprese le questioni etiche che si presentano in questa area.                      \u001b[0m\n",
              "\n",
              "\u001b[1;3;91mCRITICHE: Una critica all'idea che l'AI possa sostituire completamente i dati di mercato è che l'AI non può   \u001b[0m\n",
              "\u001b[1;3;91mancora prevedere il futuro con precisione e che ci sono molte variabili imprevisibili che possono influenzare \u001b[0m\n",
              "\u001b[1;3;91mle decisioni finanziarie. Inoltre, anche se l'AI può fornire valutazioni e raccomandazioni, la decisione      \u001b[0m\n",
              "\u001b[1;3;91mfinale spetta sempre all'uomo.                                                                                \u001b[0m\n",
              "\n",
              "\u001b[1;3;91mRISPOSTA: Sono d'accordo con entrambe le posizioni presentate, ma credo che l'AI possa avere un impatto       \u001b[0m\n",
              "\u001b[1;3;91msignificativo sulla formazione del futuro. L'AI può essere utilizzata per analizzare grandi quantità di dati e\u001b[0m\n",
              "\u001b[1;3;91mprevedere tendenze e cambiamenti nel mercato finanziario a breve e lungo termine, ma non può sostituire       \u001b[0m\n",
              "\u001b[1;3;91mcompletamente la comprensione umana delle decisioni finanziarie. Inoltre, l'AI può essere utilizzata per      \u001b[0m\n",
              "\u001b[1;3;91mcreare modelli di simulazione che consentono agli investitori di esercitarsi e migliorare le loro capacità di \u001b[0m\n",
              "\u001b[1;3;91mprevisione, senza dover affrontare i rischi reali del mercato finanziario. Tuttavia, è importante considerare \u001b[0m\n",
              "\u001b[1;3;91manche gli effetti a lungo termine dell'uso dell'AI in questo campo e affrontare le questioni etiche che si    \u001b[0m\n",
              "\u001b[1;3;91mpresentano in questa area. \u001b[0m\n",
              "\u001b[1;3;91m```\u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">Prompt: Traduci in lingua Inglese il seguente testo delimitato da tre backquotes:</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">```</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">RIFLESSIONI: L'AI e l'apprendimento automatico sono tecnologie in continua evoluzione che hanno il potenziale </span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">di trasformare molte aree della vita umana, dalle decisioni finanziarie alle cure mediche. Tuttavia, come     </span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">tutte le tecnologie, anche l'AI ha i suoi limiti e rischi. È importante considerare gli effetti a lungo       </span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">termine dell'uso dell'AI, comprese le questioni etiche che si presentano in questa area.                      </span>\n",
              "\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">CRITICHE: Una critica all'idea che l'AI possa sostituire completamente i dati di mercato è che l'AI non può   </span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">ancora prevedere il futuro con precisione e che ci sono molte variabili imprevisibili che possono influenzare </span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">le decisioni finanziarie. Inoltre, anche se l'AI può fornire valutazioni e raccomandazioni, la decisione      </span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">finale spetta sempre all'uomo.                                                                                </span>\n",
              "\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">RISPOSTA: Sono d'accordo con entrambe le posizioni presentate, ma credo che l'AI possa avere un impatto       </span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">significativo sulla formazione del futuro. L'AI può essere utilizzata per analizzare grandi quantità di dati e</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">prevedere tendenze e cambiamenti nel mercato finanziario a breve e lungo termine, ma non può sostituire       </span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">completamente la comprensione umana delle decisioni finanziarie. Inoltre, l'AI può essere utilizzata per      </span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">creare modelli di simulazione che consentono agli investitori di esercitarsi e migliorare le loro capacità di </span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">previsione, senza dover affrontare i rischi reali del mercato finanziario. Tuttavia, è importante considerare </span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">anche gli effetti a lungo termine dell'uso dell'AI in questo campo e affrontare le questioni etiche che si    </span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">presentano in questa area. </span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">```</span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;3mNumber of tokens in the  prompt: \u001b[0m\u001b[1;3;36m505\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold; font-style: italic\">Number of tokens in the  prompt: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold; font-style: italic\">505</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;3mNumber of tokens in the  answer: \u001b[0m\u001b[1;3;36m290\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold; font-style: italic\">Number of tokens in the  answer: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold; font-style: italic\">290</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "REFLECTIONS: Artificial intelligence (AI) and machine learning are constantly evolving technologies that have \n",
              "the potential to transform many aspects of human life, from financial decisions to medical care. However, like\n",
              "all technologies, AI also has its limits and risks. It is important to consider the long-term effects of using\n",
              "AI, including ethical issues that arise in this area.                                                         \n",
              "\n",
              "CRITICISM: One criticism of the idea that AI can completely replace market data is that AI cannot yet predict \n",
              "the future with precision, and there are many unpredictable variables that can influence financial decisions. \n",
              "Furthermore, even if AI can provide evaluations and recommendations, the final decision ultimately lies with  \n",
              "humans.                                                                                                       \n",
              "\n",
              "RESPONSE: I agree with both positions presented, but I believe that AI can have a significant impact on       \n",
              "shaping the future. AI can be used to analyze large amounts of data and predict trends and changes in the     \n",
              "financial market at short and long term, but it cannot completely replace human understanding of financial    \n",
              "decisions. Additionally, AI can be used to create simulation models that allow investors to practice and      \n",
              "improve their prediction skills without having to face real risks in the financial market. However, it is     \n",
              "important to consider even the long-term effects of using AI in this field and to address ethical issues that \n",
              "arise in this area.                                                                                           \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">REFLECTIONS: Artificial intelligence (AI) and machine learning are constantly evolving technologies that have \n",
              "the potential to transform many aspects of human life, from financial decisions to medical care. However, like\n",
              "all technologies, AI also has its limits and risks. It is important to consider the long-term effects of using\n",
              "AI, including ethical issues that arise in this area.                                                         \n",
              "\n",
              "CRITICISM: One criticism of the idea that AI can completely replace market data is that AI cannot yet predict \n",
              "the future with precision, and there are many unpredictable variables that can influence financial decisions. \n",
              "Furthermore, even if AI can provide evaluations and recommendations, the final decision ultimately lies with  \n",
              "humans.                                                                                                       \n",
              "\n",
              "RESPONSE: I agree with both positions presented, but I believe that AI can have a significant impact on       \n",
              "shaping the future. AI can be used to analyze large amounts of data and predict trends and changes in the     \n",
              "financial market at short and long term, but it cannot completely replace human understanding of financial    \n",
              "decisions. Additionally, AI can be used to create simulation models that allow investors to practice and      \n",
              "improve their prediction skills without having to face real risks in the financial market. However, it is     \n",
              "important to consider even the long-term effects of using AI in this field and to address ethical issues that \n",
              "arise in this area.                                                                                           \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;3;32m Generated by Vicuna-7b in \u001b[0m\u001b[1;3;32m0:11:15\u001b[0m\u001b[1;3;32m.\u001b[0m\u001b[1;3;32m467698\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold; font-style: italic\"> Generated by Vicuna-7b in 0:11:15.467698</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;5;21m ---\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff\"> ---</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"Translate into Italian the following text delimited by triple backquotes:\n",
        "```Lost in the Middle: How Language Models Use Long Contexts. Language models have become an important and flexible building block in a variety of user-facing language technologies, including conversational interfaces, search and summarization, and collaborative writing. These models perform downstream tasks primarily via prompting: all relevant task specification and data to process is formatted as a textual context, and the model returns a generated text completion. These input contexts can contain thousands of tokens, especially when using language models on lengthy inputs (e.g., legal or scientific documents, conversation histories, etc.) or augmenting them with external information (e.g.,relevant documents from a search engine, database query results, etc; Petroni et al., 2020; Ram et al., 2023; Shi et al., 2023; Mallen et al., 2023; Schick et al., 2023, inter alia). Handling these ...arly visualized in Figure 1, as we vary the position of the relevant information —language model performance is highest when relevant information occurs at the very beginning or end of its input context, and performance significantly degrades when models must access and use information in the middle of their input context (§3.3). For example, when relevant information is placed in the middle of its input context, GPT3.5-Turbo’s performance on the multi-document question task is lower than its performance when predicting without any documents (i.e., the closedbook setting; 56.1%).\n",
        "```\n",
        "\"\"\"\n",
        "prompt1 = vicunaQ4KM_CT(question)\n",
        "res = {'question': question,\n",
        "       'answer': prompt1}\n",
        "database.append(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 636
        },
        "id": "I1Qz1I4SpfEU",
        "outputId": "3fa7c51b-65a9-46c8-c6db-b178242ff622"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;3;91mPrompt: Translate into Italian the following text delimited by triple backquotes:\u001b[0m\n",
              "\u001b[1;3;91m```Lost in the Middle: How Language Models Use Long Contexts. Language models have become an important and \u001b[0m\n",
              "\u001b[1;3;91mflexible building block in a variety of user-facing language technologies, including conversational \u001b[0m\n",
              "\u001b[1;3;91minterfaces, search and summarization, and collaborative writing. These models perform downstream tasks \u001b[0m\n",
              "\u001b[1;3;91mprimarily via prompting: all relevant task specification and data to process is formatted as a textual \u001b[0m\n",
              "\u001b[1;3;91mcontext, and the model returns a generated text completion. These input contexts can contain thousands of \u001b[0m\n",
              "\u001b[1;3;91mtokens, especially when using language models on lengthy inputs \u001b[0m\u001b[1;3;91m(\u001b[0m\u001b[1;3;91me.g., legal or scientific documents, \u001b[0m\n",
              "\u001b[1;3;91mconversation histories, etc.\u001b[0m\u001b[1;3;91m)\u001b[0m\u001b[1;3;91m or augmenting them with external information \u001b[0m\u001b[1;3;91m(\u001b[0m\u001b[1;3;91me.g.,relevant documents from a \u001b[0m\n",
              "\u001b[1;3;91msearch engine, database query results, etc; Petroni et al., \u001b[0m\u001b[1;3;91m2020\u001b[0m\u001b[1;3;91m; Ram et al., \u001b[0m\u001b[1;3;91m2023\u001b[0m\u001b[1;3;91m; Shi et al., \u001b[0m\u001b[1;3;91m2023\u001b[0m\u001b[1;3;91m; Mallen \u001b[0m\n",
              "\u001b[1;3;91met al., \u001b[0m\u001b[1;3;91m2023\u001b[0m\u001b[1;3;91m; Schick et al., \u001b[0m\u001b[1;3;91m2023\u001b[0m\u001b[1;3;91m, inter alia\u001b[0m\u001b[1;3;91m)\u001b[0m\u001b[1;3;91m. Handling these \u001b[0m\u001b[1;3;91m...\u001b[0m\u001b[1;3;91marly visualized in Figure \u001b[0m\u001b[1;3;91m1\u001b[0m\u001b[1;3;91m, as we vary the \u001b[0m\n",
              "\u001b[1;3;91mposition of the relevant information —language model performance is highest when relevant information occurs \u001b[0m\n",
              "\u001b[1;3;91mat the very beginning or end of its input context, and performance significantly degrades when models must \u001b[0m\n",
              "\u001b[1;3;91maccess and use information in the middle of their input context \u001b[0m\u001b[1;3;91m(\u001b[0m\u001b[1;3;91m§\u001b[0m\u001b[1;3;91m3.3\u001b[0m\u001b[1;3;91m)\u001b[0m\u001b[1;3;91m. For example, when relevant information\u001b[0m\n",
              "\u001b[1;3;91mis placed in the middle of its input context, GPT3.\u001b[0m\u001b[1;3;91m5\u001b[0m\u001b[1;3;91m-Turbo’s performance on the multi-document question task \u001b[0m\n",
              "\u001b[1;3;91mis lower than its performance when predicting without any documents \u001b[0m\u001b[1;3;91m(\u001b[0m\u001b[1;3;91mi.e., the closedbook setting; \u001b[0m\u001b[1;3;91m56.1\u001b[0m\u001b[1;3;91m%\u001b[0m\u001b[1;3;91m)\u001b[0m\u001b[1;3;91m.\u001b[0m\n",
              "\u001b[1;3;91m```\u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">Prompt: Translate into Italian the following text delimited by triple backquotes:</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">```Lost in the Middle: How Language Models Use Long Contexts. Language models have become an important and </span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">flexible building block in a variety of user-facing language technologies, including conversational </span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">interfaces, search and summarization, and collaborative writing. These models perform downstream tasks </span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">primarily via prompting: all relevant task specification and data to process is formatted as a textual </span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">context, and the model returns a generated text completion. These input contexts can contain thousands of </span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">tokens, especially when using language models on lengthy inputs (e.g., legal or scientific documents, </span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">conversation histories, etc.) or augmenting them with external information (e.g.,relevant documents from a </span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">search engine, database query results, etc; Petroni et al., 2020; Ram et al., 2023; Shi et al., 2023; Mallen </span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">et al., 2023; Schick et al., 2023, inter alia). Handling these ...arly visualized in Figure 1, as we vary the </span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">position of the relevant information —language model performance is highest when relevant information occurs </span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">at the very beginning or end of its input context, and performance significantly degrades when models must </span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">access and use information in the middle of their input context (§3.3). For example, when relevant information</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">is placed in the middle of its input context, GPT3.5-Turbo’s performance on the multi-document question task </span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">is lower than its performance when predicting without any documents (i.e., the closedbook setting; 56.1%).</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">```</span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;3mNumber of tokens in the  prompt: \u001b[0m\u001b[1;3;36m368\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold; font-style: italic\">Number of tokens in the  prompt: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold; font-style: italic\">368</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;3mNumber of tokens in the  answer: \u001b[0m\u001b[1;3;36m431\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold; font-style: italic\">Number of tokens in the  answer: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold; font-style: italic\">431</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[48;2;39;40;34m                                                                                                              \u001b[0m\n",
              "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mPerduti nel bel mezzo: come i modelli di linguaggio utilizzano contesti lunghi. I modelli di linguaggio sono\u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
              "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdiventati un importante e flessibile blocco costruttivo in una varietà di tecnologie del linguaggio utente, \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
              "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcompresi gli interfacce conversative, la ricerca e la riassunzione, e la scrittura collaborativa. Questi \u001b[0m\u001b[48;2;39;40;34m   \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
              "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmodelli eseguono attività downstream principalmente tramite l'invio di messaggi: tutti i requisiti di task e\u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
              "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdati da elaborare sono formattati come contesto testuale, e il modello restituisce un completamento di testo\u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
              "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mgenerato. Questi contesti di input possono contenere migliaia di token, specialmente quando si utilizzano \u001b[0m\u001b[48;2;39;40;34m  \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
              "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmodelli di linguaggio su input di lunghezza (ad esempio, documenti legali o scientifici, historie delle \u001b[0m\u001b[48;2;39;40;34m    \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
              "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mconversazioni, ecc.) o li arricchiscono con informazioni esterne (ad esempio, documenti rilevanti da un moto\u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
              "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdi ricerca, risultati di query di database, ecc.). Il trattamento di questi ... visualizzato in Figure 1, co\u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
              "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mvariamo la posizione dell'informazione relevante — il rendimento del modello di linguaggio è massimo quando \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
              "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34ml'informazione relevante si trova all'inizio o alla fine del suo contesto di input, e il rendimento si degra\u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
              "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msignificativamente quando i modelli devono accedere e utilizzare le informazioni nel bel mezzo del loro \u001b[0m\u001b[48;2;39;40;34m    \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
              "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcontesto di input (§3.3). Ad esempio, quando l'informazione relevante viene posizionata nel bel mezzo del su\u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
              "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcontesto di input, il rendimento di GPT3.5-Turbo sulla task della domanda multi-documento è inferiore al suo\u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
              "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mrendimento quando prevede senza alcun documento (cioè il setting chiuso; 56,1%).\u001b[0m\u001b[48;2;39;40;34m                            \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
              "\u001b[48;2;39;40;34m                                                                                                              \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"background-color: #272822\">                                                                                                              </span>\n",
              "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">Perduti nel bel mezzo: come i modelli di linguaggio utilizzano contesti lunghi. I modelli di linguaggio sono</span><span style=\"background-color: #272822\"> </span>\n",
              "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">diventati un importante e flessibile blocco costruttivo in una varietà di tecnologie del linguaggio utente, </span><span style=\"background-color: #272822\"> </span>\n",
              "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">compresi gli interfacce conversative, la ricerca e la riassunzione, e la scrittura collaborativa. Questi </span><span style=\"background-color: #272822\">    </span>\n",
              "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">modelli eseguono attività downstream principalmente tramite l'invio di messaggi: tutti i requisiti di task e</span><span style=\"background-color: #272822\"> </span>\n",
              "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">dati da elaborare sono formattati come contesto testuale, e il modello restituisce un completamento di testo</span><span style=\"background-color: #272822\"> </span>\n",
              "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">generato. Questi contesti di input possono contenere migliaia di token, specialmente quando si utilizzano </span><span style=\"background-color: #272822\">   </span>\n",
              "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">modelli di linguaggio su input di lunghezza (ad esempio, documenti legali o scientifici, historie delle </span><span style=\"background-color: #272822\">     </span>\n",
              "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">conversazioni, ecc.) o li arricchiscono con informazioni esterne (ad esempio, documenti rilevanti da un moto</span><span style=\"background-color: #272822\"> </span>\n",
              "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">di ricerca, risultati di query di database, ecc.). Il trattamento di questi ... visualizzato in Figure 1, co</span><span style=\"background-color: #272822\"> </span>\n",
              "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">variamo la posizione dell'informazione relevante — il rendimento del modello di linguaggio è massimo quando </span><span style=\"background-color: #272822\"> </span>\n",
              "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">l'informazione relevante si trova all'inizio o alla fine del suo contesto di input, e il rendimento si degra</span><span style=\"background-color: #272822\"> </span>\n",
              "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">significativamente quando i modelli devono accedere e utilizzare le informazioni nel bel mezzo del loro </span><span style=\"background-color: #272822\">     </span>\n",
              "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">contesto di input (§3.3). Ad esempio, quando l'informazione relevante viene posizionata nel bel mezzo del su</span><span style=\"background-color: #272822\"> </span>\n",
              "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">contesto di input, il rendimento di GPT3.5-Turbo sulla task della domanda multi-documento è inferiore al suo</span><span style=\"background-color: #272822\"> </span>\n",
              "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">rendimento quando prevede senza alcun documento (cioè il setting chiuso; 56,1%).</span><span style=\"background-color: #272822\">                             </span>\n",
              "<span style=\"background-color: #272822\">                                                                                                              </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;3;32m Generated by Vicuna-7b in \u001b[0m\u001b[1;3;32m0:12:08\u001b[0m\u001b[1;3;32m.\u001b[0m\u001b[1;3;32m699118\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold; font-style: italic\"> Generated by Vicuna-7b in 0:12:08.699118</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;5;21m ---\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff\"> ---</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"Fai una predizione sulle conseguenze derivanti dall'utilizzo della Intelligenza Artificiale nel campo medico.\n",
        "Rispondi creando una lista in markdown evidenziando i punti chiave.\n",
        "\n",
        "LISTA:\n",
        "\"\"\"\n",
        "prompt1 = vicunaQ4KM_CT(question)\n",
        "res = {'question': question,\n",
        "       'answer': prompt1}\n",
        "database.append(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "xEl0xdmqtgvW",
        "outputId": "acb87b45-9550-45c3-f5c2-48e258052c1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;3;91mPrompt: Fai una predizione sulle conseguenze derivanti dall'utilizzo della Intelligenza Artificiale nel campo \u001b[0m\n",
              "\u001b[1;3;91mmedico.\u001b[0m\n",
              "\u001b[1;3;91mRispondi creando una lista in markdown evidenziando i punti chiave.\u001b[0m\n",
              "\n",
              "\u001b[1;3;91mLISTA:\u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">Prompt: Fai una predizione sulle conseguenze derivanti dall'utilizzo della Intelligenza Artificiale nel campo </span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">medico.</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">Rispondi creando una lista in markdown evidenziando i punti chiave.</span>\n",
              "\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">LISTA:</span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;3mNumber of tokens in the  prompt: \u001b[0m\u001b[1;3;36m65\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold; font-style: italic\">Number of tokens in the  prompt: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold; font-style: italic\">65</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;3mNumber of tokens in the  answer: \u001b[0m\u001b[1;3;36m315\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold; font-style: italic\">Number of tokens in the  answer: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold; font-style: italic\">315</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1;33m 1 \u001b[0mMiglioramento delle prestazioni cliniche: l'IA può aiutare a migliorare la precisione e la velocità nel    \n",
              "\u001b[1;33m   \u001b[0mdiagnostica, monitoraggio e trattamento delle malattie.                                                    \n",
              "\u001b[1;33m 2 \u001b[0mPersonalizzazione dei trattamenti: grazie all'analisi dei dati individuali, l'IA può consentire una terapia\n",
              "\u001b[1;33m   \u001b[0mpiù mirata ed efficace per ogni paziente.                                                                  \n",
              "\u001b[1;33m 3 \u001b[0mCosti sostenibili: l'uso dell'IA in medicina potrebbe portare a riduzioni dei costi sanitari grazie alla   \n",
              "\u001b[1;33m   \u001b[0mgestione del personale e delle risorse più efficiente.                                                     \n",
              "\u001b[1;33m 4 \u001b[0mPrevenzione delle malattie: l'IA può aiutare a prevedere ed evitare le malattie croniche attraverso il     \n",
              "\u001b[1;33m   \u001b[0mmonitoraggio e l'analisi dei dati di salute in tempo reale.                                                \n",
              "\u001b[1;33m 5 \u001b[0mIncremento della conoscenza scientifica: l'IA può aiutare a identificare nuove connessioni tra i dati      \n",
              "\u001b[1;33m   \u001b[0mmedici, portando allo sviluppo di nuovi farmaci ed interventi terapeutici.                                 \n",
              "\u001b[1;33m 6 \u001b[0mProtezione dei dati personali: l'IA può essere utilizzata per migliorare la sicurezza e la privacy dei dati\n",
              "\u001b[1;33m   \u001b[0msanitari attraverso l'implementazione di algoritmi di crittografia e di altre tecnologie di sicurezza.     \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 1 </span>Miglioramento delle prestazioni cliniche: l'IA può aiutare a migliorare la precisione e la velocità nel    \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>diagnostica, monitoraggio e trattamento delle malattie.                                                    \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 2 </span>Personalizzazione dei trattamenti: grazie all'analisi dei dati individuali, l'IA può consentire una terapia\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>più mirata ed efficace per ogni paziente.                                                                  \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 3 </span>Costi sostenibili: l'uso dell'IA in medicina potrebbe portare a riduzioni dei costi sanitari grazie alla   \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>gestione del personale e delle risorse più efficiente.                                                     \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 4 </span>Prevenzione delle malattie: l'IA può aiutare a prevedere ed evitare le malattie croniche attraverso il     \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>monitoraggio e l'analisi dei dati di salute in tempo reale.                                                \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 5 </span>Incremento della conoscenza scientifica: l'IA può aiutare a identificare nuove connessioni tra i dati      \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>medici, portando allo sviluppo di nuovi farmaci ed interventi terapeutici.                                 \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 6 </span>Protezione dei dati personali: l'IA può essere utilizzata per migliorare la sicurezza e la privacy dei dati\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>sanitari attraverso l'implementazione di algoritmi di crittografia e di altre tecnologie di sicurezza.     \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;3;32m Generated by Vicuna-7b in \u001b[0m\u001b[1;3;32m0:05:59\u001b[0m\u001b[1;3;32m.\u001b[0m\u001b[1;3;32m156463\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold; font-style: italic\"> Generated by Vicuna-7b in 0:05:59.156463</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;5;21m ---\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff\"> ---</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### save the logs into a PICKLE file\n",
        "\n",
        "ou can use the pickle module for that. This module has two methods,\n",
        "\n",
        "Pickling(dump): Convert Python objects into a string representation.\n",
        "Unpickling(load): Retrieving original objects from a stored string representation.\n",
        "https://docs.python.org/3.3/library/pickle.html\n",
        "\n",
        "Code:\n",
        "```\n",
        ">>> import pickle\n",
        ">>> l = [1,2,3,4]\n",
        ">>> with open(\"test\", \"wb\") as fp:   #Pickling\n",
        "...   pickle.dump(l, fp)\n",
        "...\n",
        ">>> with open(\"test\", \"rb\") as fp:   # Unpickling\n",
        "...   b = pickle.load(fp)\n",
        "...\n",
        ">>> b\n",
        "[1, 2, 3, 4]\n",
        "```\n",
        "\n",
        "Also Json\n",
        "\n",
        "dump/dumps: Serialize\n",
        "load/loads: Deserialize\n",
        "https://docs.python.org/3/library/json.html\n",
        "\n",
        "Code:\n",
        "```\n",
        ">>> import json\n",
        ">>> with open(\"test\", \"w\") as fp:\n",
        "...     json.dump(l, fp)\n",
        "...\n",
        ">>> with open(\"test\", \"r\") as fp:\n",
        "...     b = json.load(fp)\n",
        "...\n",
        ">>> b\n",
        "[1, 2, 3, 4]\n",
        "```"
      ],
      "metadata": {
        "id": "zbNwl5uhzl82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###############################\n",
        "# save the database\n",
        "###############################\n",
        "import pickle\n",
        "with open(\"databseVicuna13bITA\", \"wb\") as fp:   #Pickling\n",
        "  pickle.dump(database, fp)\n"
      ],
      "metadata": {
        "id": "rITP9RXJzmSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################\n",
        "# load the database from pickle file\n",
        "###############################\n",
        "import pickle\n",
        "with open(\"databseVicuna13bITA\", \"rb\") as fp:   # Unpickling\n",
        "  databaseVic13bITA = pickle.load(fp)"
      ],
      "metadata": {
        "id": "_wfdvLGCzmy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "databaseVic13bITA"
      ],
      "metadata": {
        "id": "B_YMmOPQzml7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZBS8sPDH2UeGLId/xp9uR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3bf67f2389904dc7ae213d20f61cca49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "warning",
            "description": "Restart Runtime",
            "disabled": false,
            "icon": "check",
            "layout": "IPY_MODEL_b9ef112dc4e648f786c53881e546b07a",
            "style": "IPY_MODEL_08a3dbac302048278e3b095c0e996901",
            "tooltip": "Click me"
          }
        },
        "b9ef112dc4e648f786c53881e546b07a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08a3dbac302048278e3b095c0e996901": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}